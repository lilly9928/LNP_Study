{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5bLDFpbomiV2",
    "outputId": "d88c2e7e-e554-4814-91ec-18f53853cb4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting konlpy\n",
      "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m19.4/19.4 MB\u001B[0m \u001B[31m53.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.9/dist-packages (from konlpy) (1.22.4)\n",
      "Collecting JPype1>=0.7.0\n",
      "  Downloading JPype1-1.4.1-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m465.3/465.3 kB\u001B[0m \u001B[31m40.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.9/dist-packages (from konlpy) (4.9.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from JPype1>=0.7.0->konlpy) (23.0)\n",
      "Installing collected packages: JPype1, konlpy\n",
      "Successfully installed JPype1-1.4.1 konlpy-0.6.0\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "#!pip install --upgrade pip setuptools wheel\n",
    "#!pip install kss\n",
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kv28-RnxlIiu",
    "outputId": "5faa427d-50a4-437e-c3b4-262628e82b7b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading RegexTokenizer: Package 'RegexTokenizer' not\n",
      "[nltk_data]     found in index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('RegexTokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zOxnCgtmj43i"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fkl7vLzWkNul",
    "outputId": "450ff6d9-62a3-4f71-e59a-6c8900ca3c7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dart', 'sounding', 'name', ',', 'Mr.Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'dheery', 'goes', 'for', 'a', 'pastry', 'shop']\n",
      "['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dart', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'dheery', 'goes', 'for', 'a', 'pastry', 'shop']\n",
      "[\"don't\", 'be', 'fooled', 'by', 'the', 'dart', 'sounding', 'name', 'mr', \"jone's\", 'orphanage', 'is', 'as', 'cheery', 'as', 'dheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Don't be fooled by the dart sounding name, Mr.Jone's Orphanage is as cheery as dheery goes for a pastry shop\"\n",
    "print(word_tokenize(text1))\n",
    "print(wordpunct_tokenize(text1))\n",
    "print(text_to_word_sequence(text1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YbpRbcxulPjX",
    "outputId": "04633480-7585-472c-c1da-7e8e79a78937"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dart', 'sounding', 'name', ',', 'Mr.Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'dheery', 'goes', 'for', 'a', 'pastry', 'shop']\n",
      "['Start', 'a', 'home-based', 'restarant', 'may', 'be', 'an', 'ideal.', 'it', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'resaurant', 'of', 'their', 'own']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "text = \"Start a home-based restarant may be an ideal. it doesn't have a food chain or resaurant of their own\"\n",
    "print(tokenizer.tokenize(text1))\n",
    "print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y2JGxJuvnjQ9",
    "outputId": "5e375724-1c77-44a7-830a-a10f16a5ff99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.Finally, the barber went up a mountain and almost to the edge of a cliff.He dug a hole in the midst of some reeds.', 'He looked about, to make sure no one was near.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "text3 = \"His barber kept his word. But keeping such a huge secret to himself was driving him crazy.\"\n",
    "text3 += \"Finally, the barber went up a mountain and almost to the edge of a cliff.\"\n",
    "text3 += \"He dug a hole in the midst of some reeds. He looked about, to make sure no one was near.\"\n",
    "\n",
    "print(sent_tokenize(text3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7wX_xQQroBkK",
    "outputId": "edebd429-8fef-4dd7-e6f1-a50265a38485"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I am actively looking for Ph.D students.', 'And you are a Ph.D student.']\n"
     ]
    }
   ],
   "source": [
    "text4 = \"I am actively looking for Ph.D students. And you are a Ph.D student.\"\n",
    "print(sent_tokenize(text4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NuaHnf94pJZX",
    "outputId": "7f22ef2d-e2fb-45b9-e0ff-4efd34151333"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['딥러닝 자연어 처리가 재미있기는 합니다.', '그런데 문제는, 영어보다 한국어로 할 때 너무 어려워요', '농담 아니에요.', '이제 해보면 알걸요?']\n",
      "['딥러닝 자연어 처리가 재미있기는 합니다.', '그런데 문제는, 영어보다 한국어로 할 때 너무 어려워요 농담 아니에요.', '이제 해보면 알걸요?']\n"
     ]
    }
   ],
   "source": [
    "import kss\n",
    "kor_text = \"딥러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 \"\n",
    "kor_text += \"너무 어려워요 농담 아니에요. 이제 해보면 알걸요? \"\n",
    "print(kss.split_sentences(kor_text))\n",
    "print(sent_tokenize(kor_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RBqQ55pnpkqf",
    "outputId": "55f8e613-5707-44ed-d0bb-4720582868b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'and', 'you', 'are', 'a', 'Ph.D.', 'student', '.']\n"
     ]
    }
   ],
   "source": [
    "text5 = \"I am actively looking for Ph.D. students. and you are a Ph.D. student.\"\n",
    "x=word_tokenize(text5)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-l_O1VPip2pG",
    "outputId": "1664b299-b1ec-4477-82ba-5ce1846cce80"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('actively', 'RB'),\n",
       " ('looking', 'VBG'),\n",
       " ('for', 'IN'),\n",
       " ('Ph.D.', 'NNP'),\n",
       " ('students', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('and', 'CC'),\n",
       " ('you', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('a', 'DT'),\n",
       " ('Ph.D.', 'NNP'),\n",
       " ('student', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "pos_tag(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EcudX4iEq70m",
    "outputId": "8ec909e1-5a87-4c3b-b535-eb71083d1313"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OKT 형태소 분석 :  ['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요']\n",
      "OKT 품사 태깅 :  [('열심히', 'Adverb'), ('코딩', 'Noun'), ('한', 'Josa'), ('당신', 'Noun'), (',', 'Punctuation'), ('연휴', 'Noun'), ('에는', 'Josa'), ('여행', 'Noun'), ('을', 'Josa'), ('가봐요', 'Verb')]\n",
      "OKT 명사 추출:  ['코딩', '당신', '연휴', '여행']\n",
      "Adverb\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt \n",
    "from konlpy.tag import Kkma\n",
    "okt = Okt()\n",
    "kkma = Kkma()\n",
    "\n",
    "example_okt_text = \"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"\n",
    "print('OKT 형태소 분석 : ',okt.morphs(example_okt_text))\n",
    "print('OKT 품사 태깅 : ',okt.pos(example_okt_text))\n",
    "print('OKT 명사 추출: ',okt.nouns(example_okt_text))\n",
    "\n",
    "print(okt.pos(example_okt_text)[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4BUIGSPRrp3l",
    "outputId": "04ad6786-8259-40a4-d3bc-fe729795418c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop_words {\"couldn't\", 'under', \"hasn't\", 'that', 'other', 'some', \"haven't\", 'needn', 'while', 'up', \"mustn't\", 'any', 'few', 'further', 'we', 'such', 't', 'her', 'doesn', 'myself', 'again', 'by', 'same', 'haven', 'm', 'against', 'ain', 'down', 'each', 'who', 'more', 'wouldn', 'but', 'from', 'all', 'own', 'these', 'out', 'y', 'won', 'if', \"that'll\", \"you'll\", \"wouldn't\", 'i', \"you'd\", 'are', 'weren', 'of', \"mightn't\", 'had', 'whom', 'mightn', 'herself', \"needn't\", 'you', \"hadn't\", 'an', 'in', 'did', 'yours', 'before', 'hadn', 'into', 'after', 'd', \"didn't\", 'and', 'their', 'them', 'at', 'very', \"doesn't\", 'himself', 'have', 'having', \"you've\", 'because', 'below', 'during', 'll', 'most', 'does', \"aren't\", 'they', 'shouldn', 'ourselves', 'which', 'once', 'your', 's', 'mustn', 'wasn', 'it', 'until', 'nor', 'our', 'off', 'can', 'too', 'doing', 'theirs', 'will', 'isn', \"wasn't\", 'on', 'itself', 'over', 'only', 'just', 'for', 'when', 'not', 'as', \"won't\", 'me', 've', 'yourselves', 'shan', 'is', 'she', 'or', 'through', 'didn', 'this', 'to', 'has', 'between', 'ours', 're', 'where', 'no', 'do', 'both', 'being', \"shan't\", 'with', 'him', \"weren't\", 'was', 'aren', 'he', 'were', \"it's\", 'so', 'yourself', 'now', 'about', 'be', 'those', 'the', \"should've\", 'my', 'than', \"isn't\", \"shouldn't\", 'ma', 'themselves', 'o', 'its', 'am', 'hasn', \"don't\", 'then', \"you're\", 'why', 'his', 'don', 'here', 'should', 'a', 'couldn', 'how', \"she's\", 'hers', 'what', 'been', 'there', 'above'}\n",
      "word_token ['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everthing', '.']\n",
      "result ['Family', 'important', 'thing', '.', 'It', \"'s\", 'everthing', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "example = \"Family is not an important thing. It's everthing. \"\n",
    "stop_words=set(stopwords.words('english'))\n",
    "\n",
    "word_tokens = word_tokenize(example)\n",
    "\n",
    "# result = []\n",
    "# for i in word_tokens:\n",
    "#   if i not in stop_words:\n",
    "#     result.append(i)\n",
    "\n",
    "\n",
    "result = [word for word in word_tokens if not word in stop_words]\n",
    "print('stop_words',stop_words)\n",
    "print('word_token',word_tokens)\n",
    "print('result',result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gDBh9dT8wKfu",
    "outputId": "38eca0ae-3288-4436-9ac3-2645763331ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 제거 전: ['고기', '를', '아무렇게나', '구', '우려', '고', '하면', '안', '돼', '.', '고기', '라고', '다', '같은', '게', '아니거든', '.', '예컨대', '삼겹살', '을', '구울', '때', '는', '중요한', '게', '있지', '.']\n",
      "불용어 제거 후: ['고기', '하면', '.', '고기', '라고', '다', '아니거든', '.', '예컨대', '삼겹살', '을', '중요한', '있지', '.']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt \n",
    "okt = Okt()\n",
    "\n",
    "kor_example =\"고기를 아무렇게나 구우려고 하면 안돼. 고기라고 다 같은 게 아니거든. \"\n",
    "kor_example += \"예컨대 삼겹살을 구울 때는 중요한 게 있지.\"\n",
    "stop_words_kor= \"를 아무렇게나 구 우려 고 안 돼 같은 게 구울 때 는\"\n",
    "\n",
    "stop_words_kor = set(stop_words_kor.split(' '))\n",
    "word_tokens = okt.morphs(kor_example)\n",
    "\n",
    "result = [word for word in word_tokens if not word in stop_words_kor]\n",
    "\n",
    "print('불용어 제거 전:',word_tokens)\n",
    "print('불용어 제거 후:',result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MirOS3Hlx3Hb",
    "outputId": "c95adc18-8ad6-4957-c94b-54ab7e215f6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['policy', 'do', 'organization', 'have', 'go', 'love', 'live', 'fly', 'die', 'watch', 'have', 'start']\n",
      "['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
      "['polici', 'do', 'organ', 'have', 'go', 'love', 'live', 'fli', 'die', 'watch', 'ha', 'start']\n",
      "['policy', 'doing', 'org', 'hav', 'going', 'lov', 'liv', 'fly', 'die', 'watch', 'has', 'start']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "n = WordNetLemmatizer()\n",
    "words = ['policy','doing','organization','have','going','love','lives','fly','dies','watched','has','starting']\n",
    "print([n.lemmatize(word,pos=\"v\") for word in words])\n",
    "print([n.lemmatize(word,pos=\"a\") for word in words])\n",
    "\n",
    "print([porters_s.stem(word)for word in words])\n",
    "print([lancaster_s.stem(word)for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oOjDnu5NzFXe",
    "outputId": "66a62315-7960-4559-f74b-1b8a00aef9a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']\n",
      "['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n",
      "['thi', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'bil', 'bon', \"'s\", 'chest', ',', 'but', 'an', 'acc', 'cop', ',', 'complet', 'in', 'al', 'thing', '--', 'nam', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'exceiv', 'of', 'the', 'red', 'cross', 'and', 'the', 'writ', 'not', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer,LancasterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text6 = \"This was not the map we found in Billy Bones's chest, but an accurate copy,\"\n",
    "text6 += \"complete in all things --names and heights and soundings --with the single exception \"\n",
    "text6 += \"of the red crosses and the written notes.\"\n",
    "\n",
    "porters_s = PorterStemmer()\n",
    "lancaster_s = LancasterStemmer()\n",
    "token = word_tokenize(text6)\n",
    "\n",
    "print(token) \n",
    "\n",
    "print([porters_s.stem(word)for word in token])\n",
    "print([lancaster_s.stem(word)for word in token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lhYHs_ZX4N8Y"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import re \n",
    "import nltk \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "axkr9k9gzWHW"
   },
   "source": [
    "HW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "id": "ljwgSBKtzXL3",
    "outputId": "379b6732-b8c1-4df2-9604-9c42cd6347d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 문서의 수: 2382\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"D:/github/LNP_Study/LNP_HW/data.csv\")\n",
    "print(\"전체 문서의 수:\", len(df));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Desc</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>author</th>\n",
       "      <th>genre</th>\n",
       "      <th>image_link</th>\n",
       "      <th>rating</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>We know that power is shifting: From West to E...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Moisés Naím</td>\n",
       "      <td>Business</td>\n",
       "      <td>https://i.gr-assets.com/images/S/compressed.ph...</td>\n",
       "      <td>3.63</td>\n",
       "      <td>The End of Power: From Boardrooms to Battlefie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Following the success of The Accidental Billio...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Blake J. Harris</td>\n",
       "      <td>Business</td>\n",
       "      <td>https://i.gr-assets.com/images/S/compressed.ph...</td>\n",
       "      <td>3.94</td>\n",
       "      <td>Console Wars: Sega, Nintendo, and the Battle t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>How to tap the power of social software and ne...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Chris Brogan</td>\n",
       "      <td>Business</td>\n",
       "      <td>https://i.gr-assets.com/images/S/compressed.ph...</td>\n",
       "      <td>3.78</td>\n",
       "      <td>Trust Agents: Using the Web to Build Influence...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>William J. Bernstein is an American financial ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>William J. Bernstein</td>\n",
       "      <td>Business</td>\n",
       "      <td>https://i.gr-assets.com/images/S/compressed.ph...</td>\n",
       "      <td>4.20</td>\n",
       "      <td>The Four Pillars of Investing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Amazing book. And I joined Steve Jobs and many...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Akio Morita</td>\n",
       "      <td>Business</td>\n",
       "      <td>https://i.gr-assets.com/images/S/compressed.ph...</td>\n",
       "      <td>4.05</td>\n",
       "      <td>Made in Japan: Akio Morita and Sony</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1                                               Desc  \\\n",
       "0             0  We know that power is shifting: From West to E...   \n",
       "1             1  Following the success of The Accidental Billio...   \n",
       "2             2  How to tap the power of social software and ne...   \n",
       "3             3  William J. Bernstein is an American financial ...   \n",
       "4             4  Amazing book. And I joined Steve Jobs and many...   \n",
       "\n",
       "   Unnamed: 0                author     genre  \\\n",
       "0         0.0           Moisés Naím  Business   \n",
       "1         1.0       Blake J. Harris  Business   \n",
       "2         2.0          Chris Brogan  Business   \n",
       "3         3.0  William J. Bernstein  Business   \n",
       "4         4.0           Akio Morita  Business   \n",
       "\n",
       "                                          image_link  rating  \\\n",
       "0  https://i.gr-assets.com/images/S/compressed.ph...    3.63   \n",
       "1  https://i.gr-assets.com/images/S/compressed.ph...    3.94   \n",
       "2  https://i.gr-assets.com/images/S/compressed.ph...    3.78   \n",
       "3  https://i.gr-assets.com/images/S/compressed.ph...    4.20   \n",
       "4  https://i.gr-assets.com/images/S/compressed.ph...    4.05   \n",
       "\n",
       "                                               title  \n",
       "0  The End of Power: From Boardrooms to Battlefie...  \n",
       "1  Console Wars: Sega, Nintendo, and the Battle t...  \n",
       "2  Trust Agents: Using the Web to Build Influence...  \n",
       "3                      The Four Pillars of Investing  \n",
       "4                Made in Japan: Akio Morita and Sony  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_9208\\3282765642.py:22: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  cleaned['cleaned'] = cleaned['cleaned'].str.replace('[^\\w\\s]','')\n"
     ]
    }
   ],
   "source": [
    "cleaned = pd.DataFrame({\n",
    "    'cleaned' : df['Desc']\n",
    "})\n",
    "\n",
    "# Remove non-ASCII characters of the \"Desc\" column.\n",
    "cleaned['cleaned'] = cleaned['cleaned'].str.replace(pat=r'[^\\w]',repl=r' ',regex=True)\n",
    "\n",
    "# Make lowercase letters of the \"Desc\" column.\n",
    "cleaned['cleaned'] = cleaned['cleaned'].str.lower()\n",
    "\n",
    "# Remove stop words of the \"Desc\" column.\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "cleaned['cleaned'] = cleaned['cleaned'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n",
    "# Remove html tags of the \"Desc\" column.\n",
    "cleaned['cleaned'] = cleaned['cleaned'].str.replace(r'<[^<>]*>', '', regex=True)\n",
    "\n",
    "# Remove punctuations of the \"Desc\" column.\n",
    "cleaned['cleaned'] = cleaned['cleaned'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "df = pd.concat([df, cleaned], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    know power shifting west east north south pres...\n",
       "1    following success accidental billionaires mone...\n",
       "2    tap power social software networks build busin...\n",
       "3    william j bernstein american financial theoris...\n",
       "4    amazing book joined steve jobs many akio morit...\n",
       "Name: cleaned, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cleaned'][:5]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
