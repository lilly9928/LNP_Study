{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'learn', 'natural', 'language', 'processing']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "token = WordPunctTokenizer().tokenize('I learn natural language processing')\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I': 0, 'learn': 1, 'natural': 2, 'language': 3, 'processing': 4}\n"
     ]
    }
   ],
   "source": [
    "word2idx = {}\n",
    "for vocab in token:\n",
    "    if vocab not in word2idx.keys():\n",
    "        word2idx[vocab] = len(word2idx)\n",
    "\n",
    "print(word2idx)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def one_hot_encoding(word,word2idx):\n",
    "    one_hot_vector = [0]*len(word2idx)\n",
    "    index = word2idx[word]\n",
    "    one_hot_vector[index] = 1\n",
    "    return one_hot_vector"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "[0, 0, 0, 1, 0]"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoding('language',word2idx)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "['I', 'learn', 'natural', 'language', 'processing']"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "for v in token:\n",
    "    print(one_hot_encoding(v,word2idx))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from lxml import etree\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# urllib.request.urlretrieve(\"https://raw.githubusercontent.com/GaoleMeng/RNN-and-FFNN-textClassification/master/ted_en-20160408.xml\", filename=\"ted_en-20160408.xml\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "targetXML=open('D:/github/LNP_Study/LNP_HW/ted_en-20160408.xml', 'r', encoding='UTF8')\n",
    "target_text = etree.parse(targetXML)\n",
    "# target_text = etree.parse('ted_en-20160408.xml')\n",
    "\n",
    "\n",
    "parse_text = '\\n'.join(target_text.xpath('//content/text()'))\n",
    "\n",
    "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n",
    "\n",
    "sent_text = sent_tokenize(content_text[:1000000])\n",
    "\n",
    "normalized_text = []\n",
    "for string in sent_text:\n",
    "    tokens = re.sub(r\"[^a-z0-9]+\",\" \",string.lower())\n",
    "    normalized_text.append(tokens)\n",
    "\n",
    "result = [word_tokenize(sentence) for sentence in normalized_text]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10975\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(result)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['here', 'are', 'two', 'reasons', 'companies', 'fail', 'they', 'only', 'do', 'more', 'of', 'the', 'same', 'or', 'they', 'only', 'do', 'what', 's', 'new']\n",
      "['to', 'me', 'the', 'real', 'real', 'solution', 'to', 'quality', 'growth', 'is', 'figuring', 'out', 'the', 'balance', 'between', 'two', 'activities', 'exploration', 'and', 'exploitation']\n",
      "['both', 'are', 'necessary', 'but', 'it', 'can', 'be', 'too', 'much', 'of', 'a', 'good', 'thing']\n"
     ]
    }
   ],
   "source": [
    "for line in result[:3]:\n",
    "    print(line)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(sentences = result, vector_size=100, window=5, min_count=5, workers=4, sg=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('woman', 0.9984459280967712), ('guy', 0.9974602460861206), ('created', 0.9971542358398438), ('week', 0.9969462752342224), ('young', 0.9968658089637756), ('number', 0.9968295693397522), ('father', 0.9965981841087341), ('written', 0.9964768290519714), ('standing', 0.9964707493782043), ('growing', 0.9964357614517212)]\n"
     ]
    }
   ],
   "source": [
    "model_result = model.wv.most_similar(\"man\")\n",
    "print(model_result)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model.wv.save_word2vec_format('eng_w2v')\n",
    "loaded_model = KeyedVectors.load_word2vec_format(\"eng_w2v\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('man', 0.998445987701416), ('created', 0.9981129169464111), ('number', 0.9978979229927063), ('american', 0.9971765875816345), ('field', 0.9968336820602417), ('growing', 0.9967020153999329), ('father', 0.9966139793395996), ('seven', 0.9966025948524475), ('week', 0.9965538382530212), ('olympics', 0.9964601993560791)]\n"
     ]
    }
   ],
   "source": [
    "model_result = loaded_model.most_similar(\"woman\")\n",
    "print(model_result)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 샘플의 개수:10975\n",
      "['here', 'are', 'two', 'reasons', 'companies', 'fail', 'they', 'only', 'do', 'more', 'of', 'the', 'same', 'or', 'they', 'only', 'do', 'what', 's', 'new']\n",
      "['to', 'me', 'the', 'real', 'real', 'solution', 'to', 'quality', 'growth', 'is', 'figuring', 'out', 'the', 'balance', 'between', 'two', 'activities', 'exploration', 'and', 'exploitation']\n",
      "['both', 'are', 'necessary', 'but', 'it', 'can', 'be', 'too', 'much', 'of', 'a', 'good', 'thing']\n"
     ]
    }
   ],
   "source": [
    "print('총 샘플의 개수:{}'.format(len(result)))\n",
    "for line in result[:3]:\n",
    "    print(line)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "model2 = FastText(result, vector_size=100, window=5, min_count=5, workers=4,sg=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('woman', 0.9496933221817017), ('na', 0.9233413934707642), ('male', 0.9217617511749268), ('scientist', 0.9133136868476868), ('female', 0.9107750058174133), ('mayor', 0.9106742143630981), ('san', 0.9096270203590393), ('artist', 0.9083265662193298), ('ran', 0.9071372151374817), ('japan', 0.9069859385490417)]\n"
     ]
    }
   ],
   "source": [
    "model_result = model2.wv.most_similar(\"man\")\n",
    "print(model_result)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model2.wv.save_word2vec_format('eng_FF')\n",
    "loaded_model2 = KeyedVectors.load_word2vec_format(\"eng_FF\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('pan', 0.9708625078201294), ('na', 0.9667264819145203), ('san', 0.9644541144371033), ('african', 0.9640722870826721), ('japan', 0.9634910821914673), ('june', 0.9628841280937195), ('yale', 0.9600289463996887), ('shaman', 0.9583501815795898), ('africa', 0.9582094550132751), ('mine', 0.957392156124115)]\n"
     ]
    }
   ],
   "source": [
    "model_result = loaded_model2.most_similar(\"woman\")\n",
    "print(model_result)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
