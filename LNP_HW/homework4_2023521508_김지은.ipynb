{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'learn', 'natural', 'language', 'processing']\n",
      "['I', 'learn', 'natural', 'language', 'processing']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "token = WordPunctTokenizer().tokenize('I learn natural language processing')\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I': 0, 'learn': 1, 'natural': 2, 'language': 3, 'processing': 4}\n",
      "{'I': 0, 'learn': 1, 'natural': 2, 'language': 3, 'processing': 4}\n"
     ]
    }
   ],
   "source": [
    "word2idx = {}\n",
    "for vocab in token:\n",
    "    if vocab not in word2idx.keys():\n",
    "        word2idx[vocab] = len(word2idx)\n",
    "\n",
    "print(word2idx)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def one_hot_encoding(word,word2idx):\n",
    "    one_hot_vector = [0]*len(word2idx)\n",
    "    index = word2idx[word]\n",
    "    one_hot_vector[index] = 1\n",
    "    return one_hot_vector"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "[0, 0, 0, 1, 0]"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "[0, 0, 0, 1, 0]"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoding('language',word2idx)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "['I', 'learn', 'natural', 'language', 'processing']"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "['I', 'learn', 'natural', 'language', 'processing']"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "for v in token:\n",
    "    print(one_hot_encoding(v,word2idx))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from lxml import etree\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# urllib.request.urlretrieve(\"https://raw.githubusercontent.com/GaoleMeng/RNN-and-FFNN-textClassification/master/ted_en-20160408.xml\", filename=\"ted_en-20160408.xml\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "targetXML=open('D:/github/LNP_Study/LNP_HW/ted_en-20160408.xml', 'r', encoding='UTF8')\n",
    "target_text = etree.parse(targetXML)\n",
    "# target_text = etree.parse('ted_en-20160408.xml')\n",
    "\n",
    "\n",
    "parse_text = '\\n'.join(target_text.xpath('//content/text()'))\n",
    "\n",
    "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n",
    "\n",
    "sent_text = sent_tokenize(content_text[:1000000])\n",
    "\n",
    "normalized_text = []\n",
    "for string in sent_text:\n",
    "    tokens = re.sub(r\"[^a-z0-9]+\",\" \",string.lower())\n",
    "    normalized_text.append(tokens)\n",
    "\n",
    "result = [word_tokenize(sentence) for sentence in normalized_text]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10975\n",
      "10975\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(result)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['here', 'are', 'two', 'reasons', 'companies', 'fail', 'they', 'only', 'do', 'more', 'of', 'the', 'same', 'or', 'they', 'only', 'do', 'what', 's', 'new']\n",
      "['to', 'me', 'the', 'real', 'real', 'solution', 'to', 'quality', 'growth', 'is', 'figuring', 'out', 'the', 'balance', 'between', 'two', 'activities', 'exploration', 'and', 'exploitation']\n",
      "['both', 'are', 'necessary', 'but', 'it', 'can', 'be', 'too', 'much', 'of', 'a', 'good', 'thing']\n",
      "['here', 'are', 'two', 'reasons', 'companies', 'fail', 'they', 'only', 'do', 'more', 'of', 'the', 'same', 'or', 'they', 'only', 'do', 'what', 's', 'new']\n",
      "['to', 'me', 'the', 'real', 'real', 'solution', 'to', 'quality', 'growth', 'is', 'figuring', 'out', 'the', 'balance', 'between', 'two', 'activities', 'exploration', 'and', 'exploitation']\n",
      "['both', 'are', 'necessary', 'but', 'it', 'can', 'be', 'too', 'much', 'of', 'a', 'good', 'thing']\n"
     ]
    }
   ],
   "source": [
    "for line in result[:3]:\n",
    "    print(line)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(sentences = result, vector_size=100, window=5, min_count=5, workers=4, sg=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('mother', 0.9987192749977112), ('project', 0.9983325600624084), ('written', 0.9978722929954529), ('number', 0.997680127620697), ('guy', 0.9976257681846619), ('small', 0.9975706934928894), ('seven', 0.9975093007087708), ('week', 0.9974759817123413), ('young', 0.9974604845046997), ('after', 0.9974386096000671)]\n",
      "[('guy', 0.9981341361999512), ('made', 0.9978157877922058), ('written', 0.9976274967193604), ('him', 0.9976024627685547), ('stage', 0.9975349307060242), ('young', 0.997450053691864), ('special', 0.9971635341644287), ('created', 0.997035026550293), ('news', 0.9970062375068665), ('small', 0.996977686882019)]\n"
     ]
    }
   ],
   "source": [
    "model_result = model.wv.most_similar(\"man\")\n",
    "print(model_result)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model.wv.save_word2vec_format('eng_w2v')\n",
    "loaded_model = KeyedVectors.load_word2vec_format(\"eng_w2v\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('week', 0.9980684518814087), ('few', 0.997696042060852), ('after', 0.9970932602882385), ('man', 0.9965399503707886), ('his', 0.9964721202850342), ('hundred', 0.9964320063591003), ('number', 0.9964286088943481), ('facebook', 0.9963241219520569), ('weeks', 0.9963095188140869), ('physics', 0.9962646961212158)]\n",
      "[('few', 0.9974390268325806), ('national', 0.9973556399345398), ('mother', 0.9967938661575317), ('weeks', 0.9967862367630005), ('his', 0.9967676401138306), ('its', 0.9967490434646606), ('business', 0.9966712594032288), ('york', 0.9966681599617004), ('movement', 0.9965611696243286), ('hundred', 0.9965486526489258)]\n"
     ]
    }
   ],
   "source": [
    "model_result = loaded_model.most_similar(\"woman\")\n",
    "print(model_result)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 샘플의 개수:10975\n",
      "['here', 'are', 'two', 'reasons', 'companies', 'fail', 'they', 'only', 'do', 'more', 'of', 'the', 'same', 'or', 'they', 'only', 'do', 'what', 's', 'new']\n",
      "['to', 'me', 'the', 'real', 'real', 'solution', 'to', 'quality', 'growth', 'is', 'figuring', 'out', 'the', 'balance', 'between', 'two', 'activities', 'exploration', 'and', 'exploitation']\n",
      "['both', 'are', 'necessary', 'but', 'it', 'can', 'be', 'too', 'much', 'of', 'a', 'good', 'thing']\n",
      "총 샘플의 개수:10975\n",
      "['here', 'are', 'two', 'reasons', 'companies', 'fail', 'they', 'only', 'do', 'more', 'of', 'the', 'same', 'or', 'they', 'only', 'do', 'what', 's', 'new']\n",
      "['to', 'me', 'the', 'real', 'real', 'solution', 'to', 'quality', 'growth', 'is', 'figuring', 'out', 'the', 'balance', 'between', 'two', 'activities', 'exploration', 'and', 'exploitation']\n",
      "['both', 'are', 'necessary', 'but', 'it', 'can', 'be', 'too', 'much', 'of', 'a', 'good', 'thing']\n"
     ]
    }
   ],
   "source": [
    "print('총 샘플의 개수:{}'.format(len(result)))\n",
    "for line in result[:3]:\n",
    "    print(line)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "model2 = FastText(result, vector_size=100, window=5, min_count=5, workers=4,sg=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('woman', 0.9593184590339661), ('shaman', 0.9165992140769958), ('pan', 0.9150781631469727), ('na', 0.91454017162323), ('male', 0.9107388257980347), ('manvendra', 0.9021128416061401), ('san', 0.9019186496734619), ('manner', 0.9012448191642761), ('map', 0.8997454047203064), ('master', 0.899082601070404)]\n",
      "[('woman', 0.9703673720359802), ('san', 0.9334506988525391), ('pan', 0.9296026229858398), ('scientist', 0.9290134906768799), ('male', 0.9260377883911133), ('na', 0.9251134991645813), ('shaman', 0.9243013858795166), ('female', 0.9227600693702698), ('number', 0.9205062389373779), ('titan', 0.9195301532745361)]\n"
     ]
    }
   ],
   "source": [
    "model_result = model2.wv.most_similar(\"man\")\n",
    "print(model_result)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model2.wv.save_word2vec_format('eng_FF')\n",
    "loaded_model2 = KeyedVectors.load_word2vec_format(\"eng_FF\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('pan', 0.9696698784828186), ('san', 0.9665348529815674), ('shaman', 0.9644757509231567), ('na', 0.9634180068969727), ('yale', 0.9604484438896179), ('man', 0.9593183994293213), ('japan', 0.9561536908149719), ('2007', 0.9561494588851929), ('june', 0.9558004140853882), ('nasa', 0.9549486637115479)]\n",
      "[('man', 0.9703673720359802), ('pan', 0.9583300352096558), ('san', 0.9555230140686035), ('na', 0.9532406330108643), ('hit', 0.9510698914527893), ('yale', 0.9499874711036682), ('shaman', 0.9496657252311707), ('lady', 0.948514461517334), ('african', 0.9463032484054565), ('june', 0.9447952508926392)]\n"
     ]
    }
   ],
   "source": [
    "model_result = loaded_model2.most_similar(\"woman\")\n",
    "print(model_result)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
